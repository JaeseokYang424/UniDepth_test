import sys
import os
import torch
import numpy as np
from PIL import Image

# --- [1] ê²½ë¡œ ë° í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ---
UNI_ENGINE_DIR = "/data2/hojun/UniDepth"
if UNI_ENGINE_DIR not in sys.path:
    sys.path.append(UNI_ENGINE_DIR)

os.environ["TORCH_HOME"] = "/data2/hojun/torch_cache"
os.environ["HUGGINGFACE_HUB_CACHE"] = "/data2/hojun/torch_cache"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"

from unidepth.models import UniDepthV2
import tensorflow as tf
import tensorflow_datasets as tfds
tf.config.set_visible_devices([], 'GPU')

# --- [2] ë¡œì»¬ OXE ë°ì´í„° ë¡œë“œ ---
def get_local_oxe_image():
    local_path = "/data2/hojun/oxe/cmu_play_fusion"
    builder = tfds.builder_from_directory(local_path)
    ds = builder.as_dataset(split='train')
    for episode in ds.take(1):
        for step in episode['steps'].take(1):
            return Image.fromarray(step['observation']['image'].numpy()).convert("RGB")

# --- [3] ë©”ì¸ ì‹¤í–‰ë¶€ ---
def main():
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # 1. ëª¨ë¸ ë¡œë“œ
    print("ğŸš€ UniDepthV2 (ViT-L) ëª¨ë¸ ë¡œë”© ì¤‘...")
    model = UniDepthV2.from_pretrained("lpiccinelli/unidepth-v2-vitl14").to(DEVICE)
    model.eval()

    # 2. ì´ë¯¸ì§€ ì¤€ë¹„
    raw_img = get_local_oxe_image()
    # ì›ë³¸ í¬ê¸° ê¸°ë¡
    orig_w, orig_h = raw_img.size
    
    # í…ì„œ ë³€í™˜ (C, H, W)
    img_tensor = torch.from_numpy(np.array(raw_img)).permute(2, 0, 1).to(DEVICE)
    # Batch ì°¨ì› ì¶”ê°€ (1, C, H, W)
    img_tensor = img_tensor.unsqueeze(0)

    # 3. Backbone ì¶œë ¥ ì¶”ì¶œ
    print("ğŸ¤– Backbone íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
    with torch.no_grad():
        # UniDepthV2ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ model.backboneì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
        # model.infer() ë‚´ë¶€ì—ì„œ ì¼ì–´ë‚˜ëŠ” ì „ì²˜ë¦¬(ì •ê·œí™” ë“±)ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì ìš©í•´ì¤ë‹ˆë‹¤.
        
        # 3-1. ëª¨ë¸ì˜ ë‚´ë¶€ í•´ìƒë„ ì„¤ì •ì— ë§ì¶° ë¦¬ì‚¬ì´ì§• (ë³´í†µ 512 ë‚´ì™¸)
        # UniDepthV2ì˜ ê²½ìš° ë‚´ë¶€ì ìœ¼ë¡œ í•´ìƒë„ë¥¼ ì¡°ì •í•˜ì—¬ backboneì— ë„£ìŠµë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” ëª¨ë¸ì´ ì‚¬ìš©í•˜ëŠ” ì‹¤ì œ ì…ë ¥ê°’ì„ ê°€ë¡œì±„ê¸° ìœ„í•´ forward_features í˜¹ì€ 
        # backboneì„ ì§ì ‘ í˜¸ì¶œí•˜ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        
        # ê°€ê³µëœ ì…ë ¥ (Normalize ë“± í¬í•¨)
        processed_input = model.preprocess(img_tensor)
        
        # Backbone í†µê³¼
        # ViT-Lì˜ ê²½ìš°, ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ íŠ¹ì§• ë§µì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        backbone_features = model.backbone(processed_input)
        
        # UniDepthV2ì˜ backbone ì¶œë ¥ì€ ë³´í†µ ë¦¬ìŠ¤íŠ¸ í˜•íƒœê±°ë‚˜ íŠœí”Œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ í…ì„œ í¬ê¸°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
        if isinstance(backbone_features, (list, tuple)):
            feature_tensor = backbone_features[-1]
        else:
            feature_tensor = backbone_features

    # 4. ê²°ê³¼ ë¹„êµ ì¶œë ¥
    print("\n" + "="*50)
    print("ğŸ“Š [Shape Transformation Result]")
    print("="*50)
    print(f"1. ì›ë³¸ ì´ë¯¸ì§€ (PIL): {orig_w} x {orig_h} (RGB)")
    print(f"2. ëª¨ë¸ ì…ë ¥ í…ì„œ:    {img_tensor.shape} (Batch, Channel, H, W)")
    print(f"3. ë°±ë³¸ í†µê³¼ í›„ (Latent): {feature_tensor.shape}")
    print("-"*50)
    
    # ë³€í™” ì„¤ëª…
    channels = feature_tensor.shape[1]
    h_feat = feature_tensor.shape[2]
    w_feat = feature_tensor.shape[3]
    
    print(f"ğŸ’¡ ë¶„ì„ ê²°ê³¼:")
    print(f" - ì±„ë„ ìˆ˜: 3 (RGB) -> {channels} (ê³ ì°¨ì› íŠ¹ì§•)")
    print(f" - ê³µê°„ í•´ìƒë„: {orig_h}x{orig_w} -> {h_feat}x{w_feat} (íŒ¨ì¹˜ ë‹¨ìœ„ ì••ì¶•)")
    print(f" - ì´ íŠ¹ì§•ì  ìˆ˜: {channels * h_feat * w_feat} ê°œ")
    print("="*50)

if __name__ == "__main__":
    main()